# Passiveâ€“Active Speaker Verification: Testing Protocols

A lightweight toolkit for running **passive** and **active** speakerâ€‘verification (SV) experiments and evaluating them with standard metrics (EER, minDCF). Utilities are included for signalâ€‘quality features (SNR/C50) and basic VAD to help you control test conditions.

---

## Why this exists

Reproducible SV evaluation depends on *how* you probe the system:

* **Passive testing**: score preâ€‘recorded trials with no interaction.
* **Active testing**: interactively collect or select trials (e.g., prompt for targeted phrases, reâ€‘tries under different SNRs) before scoring.

This repo provides both flows so you can compare protocols under the same codebase and metrics.

---

## Features

* ðŸ” **Two protocols**: `passive.py` and `active.py` pipelines.
* ðŸŽ¯ **Verification utilities**: scoring helpers in `verification.py`.
* ðŸ“‰ **Metrics**: Equal Error Rate (EER) via `compute_EER.py`, minimum DCF via `compute_min_dcf.py`.
* ðŸ”Š **Signal helpers**: SNR/C50 computation + simple VAD in `snr_c50_vad.py`.

---

## Installation

This is a plainâ€‘Python repo. Create an environment and install common audio/ML deps:

```bash
python -m venv .venv && source .venv/bin/activate   # or .venv\Scripts\activate on Windows
pip install numpy scipy scikit-learn librosa soundfile tqdm matplotlib
```

> Tip: run `python <script>.py -h` to see the exact CLI flags each script supports.

---

## Data layout (example)

You can use any SV dataset with **enrollment** and **test** utterances (e.g., VoxCeleb, inâ€‘house data). A minimal structure:

```
data/
  enroll/
    spk1/ utt1.wav, utt2.wav ...
    spk2/ ...
  test/
    spk1/ xxxxx.wav
    spk3/ ...
```

Prepare a **trials file** mapping enrollment â†” test pairs if your protocol requires it (format up to your script arguments).

---

## Quickstart

### 1) Passive protocol

Scores fixed trials without interaction.

```bash
python passive.py \
  --enroll-dir data/enroll \
  --test-dir data/test \
  --trials trials.tsv \
  --out runs/passive/scores.tsv
```

### 2) Active protocol

Collect/curate trials interactively or by rule before scoring.

```bash
python active.py \
  --enroll-dir data/enroll \
  --test-dir data/test \
  --strategy prompt \
  --out runs/active/scores.tsv
```

> Use `-h` for the actual options supported by your scripts.

---

## Evaluate

Compute **EER**:

```bash
python compute_EER.py --scores runs/passive/scores.tsv --out runs/passive/eer.txt
```

Compute **minDCF** (set target prior & costs as needed):

```bash
python compute_min_dcf.py \
  --scores runs/passive/scores.tsv \
  --p-target 0.01 --c-miss 1 --c-fa 1 \
  --out runs/passive/min_dcf.txt
```

---

## Signal quality & VAD (optional)

Estimate SNR / C50 and run a simple VAD to filter frames:

```bash
python snr_c50_vad.py --in data/test --out features/test_quality.json
```

You can then condition your active strategy on these measures (e.g., reâ€‘record if SNR < threshold).

---

## Reproducibility

* Set seeds (if the scripts expose `--seed`).
* Fix sample rate (e.g., 16 kHz) and channel layout (mono).
* Log exact CLI invocations to `runs/<...>/args.json`.

---

## Results (template)

| Protocol | EER (%) | minDCF\@p=0.01 | Notes             |
| -------: | ------: | -------------: | ----------------- |
|  Passive |       â€“ |              â€“ | baseline          |
|   Active |       â€“ |              â€“ | promptâ€‘thenâ€‘retry |

> Fill in after you run `compute_EER.py` / `compute_min_dcf.py`.

---

## Suggested repository structure

```
.
â”œâ”€â”€ active.py                # Activeâ€‘testing pipeline
â”œâ”€â”€ passive.py               # Passiveâ€‘testing pipeline
â”œâ”€â”€ verification.py          # Core verification utilities
â”œâ”€â”€ compute_EER.py           # Equal Error Rate calculator
â”œâ”€â”€ compute_min_dcf.py       # minDCF calculator
â”œâ”€â”€ snr_c50_vad.py           # SNR/C50 estimators + simple VAD
â””â”€â”€ README.md
```

---

## Contributing

Issues and PRs welcome. Please include:

* dataset description,
* exact command lines,
* environment details (`python -V`, `pip freeze`).

---

## License

*Add a LICENSE file (MIT/Apacheâ€‘2.0/BSDâ€‘3â€‘Clause are common).*

---

## Citation

If this toolkit supports a paper/report, add your BibTeX here.
